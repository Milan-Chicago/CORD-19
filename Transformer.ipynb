{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjkdRkbCfB8y",
        "colab_type": "text"
      },
      "source": [
        "# Transformers - fine tuning GPT-2 on CORD-19 abstracts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KArCn2MJfN1S",
        "colab_type": "text"
      },
      "source": [
        "### Get Huggingface's transformers, import it and other things in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rry-P9hFfLE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "\n",
        "import os\n",
        "os.chdir('/content/transformers')\n",
        "\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "\n",
        "os.chdir('/content/transformers/examples')\n",
        "\n",
        "!pip install dict_to_obj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu20rbQYffsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import run_generation\n",
        "import tensorflow as tf\n",
        "import run_language_modeling\n",
        "from dict_to_obj import DictToObj\n",
        "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, TFGPT2LMHeadModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lurA4K_fl1e",
        "colab_type": "text"
      },
      "source": [
        "### Download Abstracts and GPT-2, start fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VbUQMOvfp_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -nc -O /content/abstracts_test.txt https://raw.githubusercontent.com/PubChimps/CORD-19/master/abstractstest.txt\n",
        "!wget -nc -O /content/abstracts_train.txt https://raw.githubusercontent.com/PubChimps/CORD-19/master/abstractstrain.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_E1GKltf-5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir='./output' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2 \\\n",
        "    --num_train_epochs=1.0 \\\n",
        "    --do_train \\\n",
        "    --evaluate_during_training \\\n",
        "    --train_data_file=/content/abstracts_train.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=/content/abstracts_test.txt \\\n",
        "    --per_gpu_train_batch_size=2 \\\n",
        "    --per_gpu_eval_batch_size=2 \\\n",
        "    --block_size=512 \\\n",
        "    --gradient_accumulation_steps=5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xYWbPoZgXEC",
        "colab_type": "text"
      },
      "source": [
        "### Generate Abstracts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdqPgOSMgZSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TFGPT2LMHeadModel.from_pretrained('./test/checkpoint-1000/', pad_token_id=tokenizer.eos_token_id, from_pt=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "input_ids = tokenizer.encode('Abstract\\n\\nCovid-19', return_tensors='tf')\n",
        "sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, \n",
        "    max_length=150, \n",
        "    top_k=50, \n",
        "    top_p=0.92, \n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}